{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# FLight Data Delays and Cancelations in USA 2015\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "\n",
    "A new airline wants to expand to the United States, so they want to learn about the state of US air traffic. so they decided to create a data model of canceled and delayed flight data at US airports under active airlines in 2015. The data provided by The U.S. Department of Transportation's (DOT).\n",
    "\n",
    "This project uses data *Data 2015 Flight Delays and Cancellations*, using ETL to convert the data into a dimensional model. Serves analysis of delayed flights and flight cancellation reasons. In this project, data will be reduced to the easiest model to analyze so that users can maximize the impact of the data on flight cancellations and delays.\n",
    "\n",
    "link of data: https://www.kaggle.com/datasets/usdot/flight-delays\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "import datetime\n",
    "import psycopg2\n",
    "from pyspark.sql.functions import *\n",
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "In this project i will do an ETL data to put into a Datawarehouse. The data i am using is about Aviation(2015 Flight Delays and Cancellations).\n",
    "I choose Spark to process my data into dataware house. I am using local file to save the data as parquet file, flexible for me when datawarehouse need to scale to bigger, and i easy put it into cloud.\n",
    "Tools i use in this project is: Python, Spark\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "The data i am using in this project is about aviation. The data have the information about fligts, which contains more kind of times to flight, is the flight has cancel or not. Data has nearly 6 million of rows. This data has 3 csv file and can made dimensional models based on it.\n",
    "\n",
    "The U.S. Department of Transportation's (DOT) Bureau of Transportation Statistics tracks the on-time performance of domestic flights operated by large air carriers. Summary information on the number of on-time, delayed, canceled, and diverted flights is published in DOT's monthly Air Travel Consumer Report and in this dataset of 2015 flight delays and cancellations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark Flight ETL\") \\\n",
    "    .master('local[*]') \\\n",
    "    .config('spark.sql.execution.arrow.pyspark.enabled', True) \\\n",
    "    .config('spark.sql.session.timeZone', 'UTC') \\\n",
    "    .config('spark.driver.memory','32G') \\\n",
    "    .config('spark.ui.showConsoleProgress', True) \\\n",
    "    .config('spark.sql.repl.eagerEval.enabled', True) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airlines = spark.read.option(\"inferSchema\",True) \\\n",
    "                .option(\"delimiter\",\",\") \\\n",
    "                .option(\"header\",True) \\\n",
    "  .csv(\"airlines.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|IATA_CODE|             AIRLINE|\n",
      "+---------+--------------------+\n",
      "|       UA|United Air Lines ...|\n",
      "|       AA|American Airlines...|\n",
      "|       US|     US Airways Inc.|\n",
      "|       F9|Frontier Airlines...|\n",
      "|       B6|     JetBlue Airways|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airlines.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airports = spark.read.option(\"inferSchema\",True) \\\n",
    "                .option(\"delimiter\",\",\") \\\n",
    "                .option(\"header\",True) \\\n",
    "                .csv(\"airports.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+-----+-------+--------+----------+\n",
      "|IATA_CODE|             AIRPORT|       CITY|STATE|COUNTRY|LATITUDE| LONGITUDE|\n",
      "+---------+--------------------+-----------+-----+-------+--------+----------+\n",
      "|      ABE|Lehigh Valley Int...|  Allentown|   PA|    USA|40.65236|  -75.4404|\n",
      "|      ABI|Abilene Regional ...|    Abilene|   TX|    USA|32.41132|  -99.6819|\n",
      "|      ABQ|Albuquerque Inter...|Albuquerque|   NM|    USA|35.04022|-106.60919|\n",
      "|      ABR|Aberdeen Regional...|   Aberdeen|   SD|    USA|45.44906| -98.42183|\n",
      "|      ABY|Southwest Georgia...|     Albany|   GA|    USA|31.53552| -84.19447|\n",
      "+---------+--------------------+-----------+-----+-------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airports.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_flights = spark.read.option(\"inferSchema\",True) \\\n",
    "                .option(\"delimiter\",\",\") \\\n",
    "                .option(\"header\",True) \\\n",
    "                .csv(\"flights.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "In this data, we has some data about time in the format string with 4 character, it represent to hour and minutes, i need to make it comeback to right format. And change some columns name to right name.\n",
    "#### Cleaning Steps\n",
    "\n",
    "1. Enrich the time data columns\n",
    "\n",
    "2. Change it into right to date/timestamp format (some columns has date but just only hour and minutes)\n",
    "\n",
    "3. Change some Column name to right name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def richTime(x):\n",
    "    \"\"\"\n",
    "    function enrich to time\n",
    "    \"\"\"\n",
    "    a = len(str(x))\n",
    "    if (a == 1): return \"000\"+str(x)\n",
    "    if (a == 2): return \"00\"+str(x)\n",
    "    if (a == 3): return \"0\"+str(x)\n",
    "    return str(x)\n",
    "\n",
    "richTimeUDF = udf(lambda x: richTime(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_flights = df_flights .withColumn(\"SCHEDULED_DEPARTURE\",richTimeUDF(df_flights.SCHEDULED_DEPARTURE))\\\n",
    "                        .withColumn(\"DEPARTURE_TIME\",richTimeUDF(df_flights.DEPARTURE_TIME))\\\n",
    "                        .withColumn(\"WHEELS_OFF\",richTimeUDF(df_flights.WHEELS_OFF))\\\n",
    "                        .withColumn(\"WHEELS_ON\",richTimeUDF(df_flights.WHEELS_ON))\\\n",
    "                        .withColumn(\"SCHEDULED_ARRIVAL\",richTimeUDF(df_flights.SCHEDULED_ARRIVAL))\\\n",
    "                        .withColumn(\"ARRIVAL_TIME\",richTimeUDF(df_flights.ARRIVAL_TIME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_flights = df_flights.withColumn('schedule_dep_ts',\n",
    "                                    concat(df_flights.DAY,\n",
    "                                    lit('/'),df_flights.MONTH,\n",
    "                                    lit('/'),df_flights.YEAR,\n",
    "                                    lit(' '),\n",
    "                                    substring(df_flights.SCHEDULED_DEPARTURE,1,2),\n",
    "                                    lit(':'),\n",
    "                                    substring(df_flights.SCHEDULED_DEPARTURE,3,2),\n",
    "                                    lit(':00')))\\\n",
    "                        .withColumn('departure_hour',\n",
    "                                    concat(substring(df_flights.DEPARTURE_TIME,1,2),\n",
    "                                    lit(':'),\n",
    "                                    substring(df_flights.DEPARTURE_TIME,3,2),\n",
    "                                    lit(':00')))\\\n",
    "                        .withColumn('wheels_off_hour',\n",
    "                                    concat(substring(df_flights.WHEELS_OFF,1,2),\n",
    "                                    lit(':'),\n",
    "                                    substring(df_flights.WHEELS_OFF,3,2),\n",
    "                                    lit(':00')))\\\n",
    "                        .withColumn('wheels_on_hour',\n",
    "                                    concat(substring(df_flights.WHEELS_ON,1,2),\n",
    "                                    lit(':'),\n",
    "                                    substring(df_flights.WHEELS_ON,3,2),\n",
    "                                    lit(':00')))\\\n",
    "                        .withColumn('schedule_arr_hour',\n",
    "                                    concat(substring(df_flights.SCHEDULED_ARRIVAL,1,2),\n",
    "                                    lit(':'),\n",
    "                                    substring(df_flights.SCHEDULED_ARRIVAL,3,2),\n",
    "                                    lit(':00')))\\\n",
    "                        .withColumn('arrival_hour',\n",
    "                                    concat(substring(df_flights.ARRIVAL_TIME,1,2),\n",
    "                                    lit(':'),\n",
    "                                    substring(df_flights.ARRIVAL_TIME,3,2),\n",
    "                                    lit(':00')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_flights = df_flights.withColumn('SCHEDULED_DEP_TS',to_timestamp('schedule_dep_ts', format='dd/MM/yyyy HH:mm:ss'))\\\n",
    "                        .withColumn('DEPARTURE_HOUR',date_format('departure_hour', format='HH:mm:ss'))\\\n",
    "                        .withColumn('WHEELS_OFF_HOUR',date_format('wheels_off_hour', format='HH:mm:ss'))\\\n",
    "                        .withColumn('WHEELS_ON_HOUR',date_format('wheels_on_hour', format='HH:mm:ss'))\\\n",
    "                        .withColumn('SCHEDULED_ARR_HOUR',date_format('schedule_arr_hour', format='HH:mm:ss'))\\\n",
    "                        .withColumn('ARRIVAL_HOUR',date_format('arrival_hour', format='HH:mm:ss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------+---------------+--------------+------------------+------------+\n",
      "|   SCHEDULED_DEP_TS|SCHEDULED_DEPARTURE|DEPARTURE_HOUR|WHEELS_OFF_HOUR|WHEELS_ON_HOUR|SCHEDULED_ARR_HOUR|ARRIVAL_HOUR|\n",
      "+-------------------+-------------------+--------------+---------------+--------------+------------------+------------+\n",
      "|2015-01-01 00:05:00|               0005|      23:54:00|       00:15:00|      04:04:00|          04:30:00|    04:08:00|\n",
      "+-------------------+-------------------+--------------+---------------+--------------+------------------+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_flights.select('SCHEDULED_DEP_TS','SCHEDULED_DEPARTURE','DEPARTURE_HOUR','WHEELS_OFF_HOUR','WHEELS_ON_HOUR','SCHEDULED_ARR_HOUR','ARRIVAL_HOUR').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_flights = df_flights.withColumnRenamed('AIRLINE','AIRLINE_IATA')\\\n",
    "                        .withColumnRenamed('SCHEDULED_TIME','SCHEDULED_AIR_TIME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "I use Star Schema Data Model. It suitable to this data and easy to processing the data\n",
    "\n",
    "![alt text](schema.png \"Database Schema\")\n",
    "\n",
    "This is Dimensional model for this project. This model has one fact table is Flight has data about Flight(time, delay, cancelation, other information)\n",
    "\n",
    "For match with fact table we have 3 dimension table:\n",
    "\n",
    "* Airline: save the information about airline with iata code\n",
    "\n",
    "* Time: time of flight (day, month, year, day of week)\n",
    "\n",
    "* Airport: save the information about airport in USA\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "The step load data into Data Warehouse\n",
    "1. Join flight data with airport data to load data into airport_table\n",
    "\n",
    "2. Join flight data with airline data to load data into airline_table\n",
    "\n",
    "3. Load data into time_table from flight data\n",
    "\n",
    "4. Load data into fact table (flight) from flight data\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Join flight data with airport data to load data into airport_table\n",
    "airport_list = df_flights.selectExpr('ORIGIN_AIRPORT as AIRPORT_IATA').distinct().collect() + df_flights.selectExpr('DESTINATION_AIRPORT as AIRPORT_IATA').distinct().collect()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('AIRPORT_IATA', StringType(), True)\n",
    "])\n",
    "airport_df = spark.createDataFrame(airport_list, schema)\n",
    "airport_table = airport_df.select('AIRPORT_IATA').distinct()\\\n",
    "                          .join(df_airports ,(airport_df.AIRPORT_IATA==df_airports.IATA_CODE) , how='left_outer')\\\n",
    "                          .select( 'AIRPORT_IATA',\n",
    "                                   'AIRPORT',\n",
    "                                   'CITY',\n",
    "                                   'STATE',\n",
    "                                   'COUNTRY',\n",
    "                                   'LATITUDE',\n",
    "                                   'LONGITUDE').distinct()\n",
    "airport_table.write.mode('overwrite').parquet('DIMENTION_TABLE/' + 'AIRPORT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Join flight data with airline data to load data into airline_table\n",
    "df_airlines = df_airlines.withColumnRenamed('AIRLINE', 'AIRLINE_NAME')\n",
    "\n",
    "airline_table = df_flights.select('AIRLINE_IATA').distinct()\\\n",
    "                          .join(df_airlines ,(df_flights.AIRLINE_IATA==df_airlines.IATA_CODE) , how='left_outer')\\\n",
    "                          .select( 'AIRLINE_IATA',\n",
    "                                   'AIRLINE_NAME'\n",
    "                                 ).distinct()\n",
    "airline_table.write.mode('overwrite').parquet('DIMENTION_TABLE/' + 'AIRLINE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load data into time_table from flight data\n",
    "time_table = df_flights.select('SCHEDULED_DEP_TS','DAY','MONTH','YEAR','DAY_OF_WEEK').distinct()\n",
    "time_table.write.mode('overwrite').parquet('DIMENTION_TABLE/' + 'TIME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load data into fact table (flight) from flight data\n",
    "flight_Table = df_flights.select('SCHEDULED_DEP_TS','ORIGIN_AIRPORT','DESTINATION_AIRPORT','AIRLINE_IATA','FLIGHT_NUMBER','TAIL_NUMBER'\\\n",
    "                                 ,'SCHEDULED_AIR_TIME','SCHEDULED_ARR_HOUR','DEPARTURE_HOUR','AIR_TIME','ARRIVAL_HOUR','ELAPSED_TIME'\\\n",
    "                                 ,'DEPARTURE_DELAY','ARRIVAL_DELAY','TAXI_OUT','WHEELS_OFF_HOUR','WHEELS_ON_HOUR','TAXI_IN'\\\n",
    "                                ,'DISTANCE','CANCELLED','DIVERTED','CANCELLATION_REASON').distinct()\\\n",
    "                                .withColumn('FLIGHT_ID', monotonically_increasing_id())\n",
    "flight_Table.write.mode('overwrite').parquet('FACT_TABLE/' + 'FLIGHT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "Data quality check contain:\n",
    "\n",
    "- hasrows: check each table has more than 0 rows, ensure data has loaded in to datawarehouse.\n",
    "\n",
    "- check_integrity: check integrity between tables in datawarehouse, make sure the integrity of data is grasping.\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def has_rows(dataframe):\n",
    "    \"\"\"\n",
    "    Checks has data in dataframe\n",
    "    \"\"\"\n",
    "    record_num = dataframe.count()\n",
    "    if record_num <= 0:\n",
    "        raise ValueError('This table is empty!!!')\n",
    "    print('This table has '+str(record_num)+' records')\n",
    "\n",
    "    \n",
    "def check_integrity(flight, airport, airline, time):\n",
    "    \"\"\"\n",
    "    Checks integrity data in dataframe\n",
    "    input: fact table and 3 dimension table\n",
    "    \"\"\"\n",
    "    check_airport_destination = flight.select('DESTINATION_AIRPORT').distinct() \\\n",
    "                     .join(airport, flight.DESTINATION_AIRPORT == airport.AIRPORT_IATA, \"left_anti\") \\\n",
    "                     .count() == 0\n",
    "    check_airport_origin = flight.select('ORIGIN_AIRPORT').distinct() \\\n",
    "                     .join(airport, flight.ORIGIN_AIRPORT == airport.AIRPORT_IATA, \"left_anti\") \\\n",
    "                     .count() == 0\n",
    "    check_time = flight.select('SCHEDULED_DEP_TS').distinct() \\\n",
    "                     .join(time, flight.SCHEDULED_DEP_TS == time.SCHEDULED_DEP_TS, \"left_anti\") \\\n",
    "                     .count() == 0\n",
    "    check_airline = flight.select('AIRLINE_IATA').distinct() \\\n",
    "                     .join(airline, flight.AIRLINE_IATA == airline.AIRLINE_IATA, \"left_anti\") \\\n",
    "                     .count() == 0\n",
    "    if not check_airport_destination & check_time & check_airline & check_airport_origin:\n",
    "        raise ValueError('Error at integrity!!!')\n",
    "    print('Check Integrity passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#load data from datawarehouse\n",
    "flight = spark.read.parquet('FACT_TABLE/' + 'FLIGHT')\n",
    "airport = spark.read.parquet('DIMENTION_TABLE/' + 'AIRPORT')\n",
    "airline = spark.read.parquet('DIMENTION_TABLE/' + 'AIRLINE')\n",
    "time = spark.read.parquet('DIMENTION_TABLE/' + 'TIME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This table has 5819079 records\n"
     ]
    }
   ],
   "source": [
    "has_rows(flight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This table has 629 records\n"
     ]
    }
   ],
   "source": [
    "has_rows(airport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This table has 14 records\n"
     ]
    }
   ],
   "source": [
    "has_rows(airline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This table has 397568 records\n"
     ]
    }
   ],
   "source": [
    "has_rows(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check Integrity passed\n"
     ]
    }
   ],
   "source": [
    "check_integrity(flight,airport,airline,time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "##### Flight_table\n",
    "\n",
    "* FLIGHT_ID: Flight log Identifier\n",
    "* SCHEDULED_DEP_TS: Planned Departure Time (timestamp)\n",
    "* ORIGIN_AIRPORT: Starting Airport Iata code\n",
    "* DESTINATION_AIRPORT: Destination Airport Iata code\n",
    "* AIRLINE_IATA: Airline Identifier Iata code\n",
    "* FLIGHT_NUMBER: Flight Identifier\n",
    "* TAIL_NUMBER: Aircraft Identifier\n",
    "* SCHEDULED_AIR_TIME: Planned time amount needed for the flight trip\n",
    "* SCHEDULED_ARR_HOUR: Planned arrival time\n",
    "* DEPARTURE_HOUR: WHEEL_OFF - TAXI_OUT, time flight start run in street\n",
    "* AIR_TIME: The time duration between wheels_off and wheels_on time, the time fly\n",
    "* ARRIVAL_HOUR: WHEELS_ON+TAXI_IN, the time ending of taxi in, when aircraft stop\n",
    "* ELAPSED_TIME: AIR_TIME+TAXI_IN+TAXI_OUT\n",
    "* DEPARTURE_DELAY: Total Delay on Departure\n",
    "* ARRIVAL_DELAY: ARRIVAL_HOUR - SCHEDULED_ARR_HOUR\n",
    "* TAXI_OUT: The time duration elapsed between departure from the origin airport gate and wheels off\n",
    "* WHEELS_OFF_HOUR: The time point that the aircraft's wheels leave the ground\n",
    "* WHEELS_ON_HOUR: The time point that the aircraft's wheels touch on the ground\n",
    "* TAXI_IN: The time duration elapsed between wheels-on and gate arrival at the destination airport\n",
    "* DISTANCE: Distance between two airports\n",
    "* CANCELLED: Flight Cancelled (1 = cancelled)\n",
    "* DIVERTED: Aircraft landed on airport that out of schedule\n",
    "* CANCELLATION_REASON: Reason for Cancellation of flight: A - Airline/Carrier; B - Weather; C - National Air System; D - Security\n",
    "\n",
    "##### Airline_table\n",
    "\n",
    "* AIRLINE_IATA: Airline Identifier Iata code\n",
    "* AIRLINE_NAME: name of Airline\n",
    "\n",
    "##### Airport_table\n",
    "\n",
    "* AIRPORT_IATA: Airport Location Identifier\n",
    "* AIRPORT: Airport's Name\n",
    "* CITY: city of airport location\n",
    "* STATE: state of airport location\n",
    "* COUNTRY: country of airport location\n",
    "* LATITUDE: latitude of airport location\n",
    "* LONGITUDE: longitude of airport location\n",
    "\n",
    "##### Time_table\n",
    "\n",
    "* SCHEDULED_DEP_TS: Planned Departure Time (timestamp)\n",
    "* DAY: day of flight log\n",
    "* MONTH: month of flight log\n",
    "* YEAR: year of flight log\n",
    "* DAY_OF_WEEK: day of week of flight log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "1. In this Project, I use Apache Spark to process and save data in parquet file. Spark is a powerful tool for big data processing, it can meet all my needs in file processing. Saving files as parquet reduces storage space and is easier to expand with larger amounts of data. Furthermore, saving the parquet file allows me to easily convert to another storage system (e.g. Amazon S3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "2. The data must be updated daily, as a date partition can be used for the data and it will always be up to date. The amount of data processed in a day will also be limited to the number of flights in the US or the world, so the data will not be overloaded when updating daily (even monthly, annually)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* The data was increased by 100x -> When the data is multiplied by 100, spark can still process it fine and the parquet file's storage space still significantly reduces the storage memory, maybe consider moving the data to a cloud service like Amazon S3, will not a great cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* The data populates a dashboard that must be updated on a daily basis by 7am every day => This is completely simple as we can use Apache Airflow to configure the whole thing to do with daily data. Using DAGs and set data quality, we just waiting for email success or fail to monitoring the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* The database needed to be accessed by 100+ people => we can use Redshift to handle this case as it has auto-scaling and strong support for read performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
